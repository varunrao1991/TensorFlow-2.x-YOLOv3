{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj3eAw1OXOnB"
   },
   "source": [
    "**Connect and authorize google drive with google colab:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 7965,
     "status": "ok",
     "timestamp": 1601446634334,
     "user": {
      "displayName": "Python Lessons",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgMQmMhFapKcavl337-vY17yrbowBHBlZQ5qYQv=s64",
      "userId": "12382394757900236362"
     },
     "user_tz": -180
    },
    "id": "M3cWo7hhc-qO",
    "outputId": "1a7edf1a-a3a9-450e-ace2-34bf1ad2c6a8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, ZeroPadding2D, BatchNormalization, MaxPool2D, Add, Concatenate\n",
    "from yolov3.utils import read_class_names\n",
    "from yolov3.configs import *\n",
    "\n",
    "STRIDES = np.array(YOLO_STRIDES)\n",
    "ANCHORS = (np.array(YOLO_ANCHORS).T/STRIDES).T\n",
    "\n",
    "class BatchNormalization(BatchNormalization):\n",
    "    # \"Frozen state\" and \"inference mode\" are two separate concepts.\n",
    "    # `layer.trainable = False` is to freeze the layer, so the layer will use\n",
    "    # stored moving `var` and `mean` in the \"inference mode\", and both `gama`\n",
    "    # and `beta` will not be updated !\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            training = tf.constant(False)\n",
    "        training = tf.logical_and(training, self.trainable)\n",
    "        return super().call(x, training)\n",
    "\n",
    "def convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True):\n",
    "    if downsample:\n",
    "        input_layer = ZeroPadding2D(((1, 0), (1, 0)))(input_layer)\n",
    "        padding = 'valid'\n",
    "        strides = 2\n",
    "    else:\n",
    "        strides = 1\n",
    "        padding = 'same'\n",
    "\n",
    "    conv = Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], strides=strides,\n",
    "                  padding=padding, use_bias=not bn)(input_layer)\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    if activate == True:\n",
    "        conv = LeakyReLU(alpha=0.1)(conv)\n",
    "\n",
    "    return conv\n",
    "\n",
    "def residual_block(input_layer, input_channel, filter_num1, filter_num2):\n",
    "    short_cut = input_layer\n",
    "    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1))\n",
    "    conv = convolutional(conv       , filters_shape=(3, 3, filter_num1,   filter_num2))\n",
    "\n",
    "    residual_output = Add()([short_cut, conv])\n",
    "    return residual_output\n",
    "\n",
    "def upsample(input_layer):\n",
    "    return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='nearest')\n",
    "\n",
    "\n",
    "def darknet53(input_data):\n",
    "    input_data = convolutional(input_data, (3, 3,  3,  32))\n",
    "    input_data = convolutional(input_data, (3, 3, 32,  64), downsample=True)\n",
    "\n",
    "    for i in range(1):\n",
    "        input_data = residual_block(input_data,  64,  32, 64)\n",
    "\n",
    "    input_data = convolutional(input_data, (3, 3,  64, 128), downsample=True)\n",
    "\n",
    "    for i in range(2):\n",
    "        input_data = residual_block(input_data, 128,  64, 128)\n",
    "\n",
    "    input_data = convolutional(input_data, (3, 3, 128, 256), downsample=True)\n",
    "\n",
    "    for i in range(8):\n",
    "        input_data = residual_block(input_data, 256, 128, 256)\n",
    "\n",
    "    route_1 = input_data\n",
    "    input_data = convolutional(input_data, (3, 3, 256, 512), downsample=True)\n",
    "\n",
    "    for i in range(8):\n",
    "        input_data = residual_block(input_data, 512, 256, 512)\n",
    "\n",
    "    route_2 = input_data\n",
    "    input_data = convolutional(input_data, (3, 3, 512, 1024), downsample=True)\n",
    "\n",
    "    for i in range(4):\n",
    "        input_data = residual_block(input_data, 1024, 512, 1024)\n",
    "\n",
    "    return route_1, route_2, input_data\n",
    "\n",
    "def YOLOv3(input_layer, NUM_CLASS):\n",
    "    route_1, route_2, conv = darknet53(input_layer)\n",
    "\n",
    "    conv = convolutional(conv, (1, 1, 1024,  512))\n",
    "    conv = convolutional(conv, (3, 3,  512, 1024))\n",
    "    conv = convolutional(conv, (1, 1, 1024,  512))\n",
    "    conv = convolutional(conv, (3, 3,  512, 1024))\n",
    "    conv = convolutional(conv, (1, 1, 1024,  512))\n",
    "    conv_lobj_branch = convolutional(conv, (3, 3, 512, 1024))\n",
    "    \n",
    "    # conv_lbbox is used to predict large-sized objects , Shape = [None, 13, 13, 255] \n",
    "    conv_lbbox = convolutional(conv_lobj_branch, (1, 1, 1024, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n",
    "\n",
    "    conv = convolutional(conv, (1, 1,  512,  256))\n",
    "    # upsample here uses the nearest neighbor interpolation method, which has the advantage that the\n",
    "    # upsampling process does not need to learn, thereby reducing the network parameter  \n",
    "    conv = upsample(conv)\n",
    "\n",
    "    conv = Concatenate()([conv, route_2])\n",
    "    conv = convolutional(conv, (1, 1, 768, 256))\n",
    "    conv = convolutional(conv, (3, 3, 256, 512))\n",
    "    conv = convolutional(conv, (1, 1, 512, 256))\n",
    "    conv = convolutional(conv, (3, 3, 256, 512))\n",
    "    conv = convolutional(conv, (1, 1, 512, 256))\n",
    "    conv_mobj_branch = convolutional(conv, (3, 3, 256, 512))\n",
    "\n",
    "    # conv_mbbox is used to predict medium-sized objects, shape = [None, 26, 26, 255]\n",
    "    conv_mbbox = convolutional(conv_mobj_branch, (1, 1, 512, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n",
    "\n",
    "    conv = convolutional(conv, (1, 1, 256, 128))\n",
    "    conv = upsample(conv)\n",
    "\n",
    "    conv = Concatenate()([conv, route_1])\n",
    "    conv = convolutional(conv, (1, 1, 384, 128))\n",
    "    conv = convolutional(conv, (3, 3, 128, 256))\n",
    "    conv = convolutional(conv, (1, 1, 256, 128))\n",
    "    conv = convolutional(conv, (3, 3, 128, 256))\n",
    "    conv = convolutional(conv, (1, 1, 256, 128))\n",
    "    conv_sobj_branch = convolutional(conv, (3, 3, 128, 256))\n",
    "    \n",
    "    # conv_sbbox is used to predict small size objects, shape = [None, 52, 52, 255]\n",
    "    conv_sbbox = convolutional(conv_sobj_branch, (1, 1, 256, 3*(NUM_CLASS +5)), activate=False, bn=False)\n",
    "\n",
    "    print(\"conv_sbbox\", conv_sbbox.name, conv_sbbox.shape)\n",
    "    print(\"conv_mbbox\", conv_mbbox.name, conv_mbbox.shape)\n",
    "    print(\"conv_lbbox\", conv_lbbox.name, conv_lbbox.shape)\n",
    "    return [conv_sbbox, conv_mbbox, conv_lbbox]\n",
    "\n",
    "def Create_Yolov3(input_size=416, channels=3, training=False, CLASSES=YOLO_COCO_CLASSES):\n",
    "    NUM_CLASS = len(read_class_names(CLASSES))\n",
    "    input_layer  = Input([input_size, input_size, channels])\n",
    "\n",
    "    conv_tensors = YOLOv3(input_layer, NUM_CLASS)\n",
    "\n",
    "    output_tensors = []\n",
    "    for i, conv_tensor in enumerate(conv_tensors):\n",
    "        pred_tensor = decode(conv_tensor, NUM_CLASS, i)\n",
    "        output_tensors.append(pred_tensor)\n",
    "\n",
    "    YoloV3 = tf.keras.Model(input_layer, output_tensors)\n",
    "    return YoloV3\n",
    "\n",
    "def decode(conv_output, NUM_CLASS, i=0):\n",
    "    # where i = 0, 1 or 2 to correspond to the three grid scales  \n",
    "    conv_shape       = tf.shape(conv_output)\n",
    "    batch_size       = conv_shape[0]\n",
    "    output_size      = conv_shape[1]\n",
    "\n",
    "    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n",
    "\n",
    "    conv_raw_dxdy = conv_output[:, :, :, :, 0:2] # offset of center position     \n",
    "    conv_raw_dwdh = conv_output[:, :, :, :, 2:4] # Prediction box length and width offset\n",
    "    conv_raw_conf = conv_output[:, :, :, :, 4:5] # confidence of the prediction box\n",
    "    conv_raw_prob = conv_output[:, :, :, :, 5: ] # category probability of the prediction box \n",
    "\n",
    "    # next need Draw the grid. Where output_size is equal to 13, 26 or 52  \n",
    "    y = tf.range(output_size, dtype=tf.int32)\n",
    "    y = tf.expand_dims(y, -1)\n",
    "    y = tf.tile(y, [1, output_size])\n",
    "    x = tf.range(output_size,dtype=tf.int32)\n",
    "    x = tf.expand_dims(x, 0)\n",
    "    x = tf.tile(x, [output_size, 1])\n",
    "\n",
    "    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n",
    "    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])\n",
    "    xy_grid = tf.cast(xy_grid, tf.float32)\n",
    "\n",
    "    # Calculate the center position of the prediction box:\n",
    "    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]\n",
    "    # Calculate the length and width of the prediction box:\n",
    "    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]\n",
    "\n",
    "    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n",
    "    pred_conf = tf.sigmoid(conv_raw_conf) # object box calculates the predicted confidence\n",
    "    pred_prob = tf.sigmoid(conv_raw_prob) # calculating the predicted probability category box object\n",
    "\n",
    "    # calculating the predicted probability category box object\n",
    "    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "#\n",
    "#   File name   : utils.py\n",
    "#   Author      : PyLessons\n",
    "#   Created date: 2020-09-27\n",
    "#   Website     : https://pylessons.com/\n",
    "#   GitHub      : https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3\n",
    "#   Description : additional yolov3 and yolov4 functions\n",
    "#\n",
    "#================================================================\n",
    "from multiprocessing import Process, Queue, Pipe\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import colorsys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from yolov3.configs import *\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "def image_preprocess(image, target_size, gt_boxes=None):\n",
    "    ih, iw    = target_size\n",
    "    h,  w, _  = image.shape\n",
    "\n",
    "    scale = min(iw/w, ih/h)\n",
    "    nw, nh  = int(scale * w), int(scale * h)\n",
    "    image_resized = cv2.resize(image, (nw, nh))\n",
    "\n",
    "    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n",
    "    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n",
    "    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n",
    "    image_paded = image_paded / 255.\n",
    "\n",
    "    if gt_boxes is None:\n",
    "        return image_paded\n",
    "\n",
    "    else:\n",
    "        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\n",
    "        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\n",
    "        return image_paded, gt_boxes\n",
    "\n",
    "\n",
    "def draw_bbox(image, bboxes, CLASSES=YOLO_COCO_CLASSES, show_label=True, show_confidence = True, Text_colors=(255,255,0), rectangle_colors='', tracking=False):   \n",
    "    NUM_CLASS = read_class_names(CLASSES)\n",
    "    num_classes = len(NUM_CLASS)\n",
    "    image_h, image_w, _ = image.shape\n",
    "    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "    #print(\"hsv_tuples\", hsv_tuples)\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "\n",
    "    random.seed(0)\n",
    "    random.shuffle(colors)\n",
    "    random.seed(None)\n",
    "\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        coor = np.array(bbox[:4], dtype=np.int32)\n",
    "        score = bbox[4]\n",
    "        class_ind = int(bbox[5])\n",
    "        bbox_color = rectangle_colors if rectangle_colors != '' else colors[class_ind]\n",
    "        bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n",
    "        if bbox_thick < 1: bbox_thick = 1\n",
    "        fontScale = 0.75 * bbox_thick\n",
    "        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
    "\n",
    "        # put object rectangle\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)\n",
    "\n",
    "        if show_label:\n",
    "            # get text label\n",
    "            score_str = \" {:.2f}\".format(score) if show_confidence else \"\"\n",
    "\n",
    "            if tracking: score_str = \" \"+str(score)\n",
    "\n",
    "            try:\n",
    "                label = \"{}\".format(NUM_CLASS[class_ind]) + score_str\n",
    "            except KeyError:\n",
    "                print(\"You received KeyError, this might be that you are trying to use yolo original weights\")\n",
    "                print(\"while using custom classes, if using custom model in configs.py set YOLO_CUSTOM_WEIGHTS = True\")\n",
    "\n",
    "            # get text size\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                                                                  fontScale, thickness=bbox_thick)\n",
    "            # put filled text rectangle\n",
    "            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n",
    "\n",
    "            # put text above rectangle\n",
    "            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                        fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def bboxes_iou(boxes1, boxes2):\n",
    "    boxes1 = np.array(boxes1)\n",
    "    boxes2 = np.array(boxes2)\n",
    "\n",
    "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
    "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
    "\n",
    "    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n",
    "    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
    "\n",
    "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
    "    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n",
    "    union_area    = boxes1_area + boxes2_area - inter_area\n",
    "    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\n",
    "\n",
    "    return ious\n",
    "\n",
    "\n",
    "def nms(bboxes, iou_threshold, sigma=0.3, method='nms'):\n",
    "    \"\"\"\n",
    "    :param bboxes: (xmin, ymin, xmax, ymax, score, class)\n",
    "\n",
    "    Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf\n",
    "          https://github.com/bharatsingh430/soft-nms\n",
    "    \"\"\"\n",
    "    classes_in_img = list(set(bboxes[:, 5]))\n",
    "    best_bboxes = []\n",
    "\n",
    "    for cls in classes_in_img:\n",
    "        cls_mask = (bboxes[:, 5] == cls)\n",
    "        cls_bboxes = bboxes[cls_mask]\n",
    "        # Process 1: Determine whether the number of bounding boxes is greater than 0 \n",
    "        while len(cls_bboxes) > 0:\n",
    "            # Process 2: Select the bounding box with the highest score according to socre order A\n",
    "            max_ind = np.argmax(cls_bboxes[:, 4])\n",
    "            best_bbox = cls_bboxes[max_ind]\n",
    "            best_bboxes.append(best_bbox)\n",
    "            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n",
    "            # Process 3: Calculate this bounding box A and\n",
    "            # Remain all iou of the bounding box and remove those bounding boxes whose iou value is higher than the threshold \n",
    "            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n",
    "            weight = np.ones((len(iou),), dtype=np.float32)\n",
    "\n",
    "            assert method in ['nms', 'soft-nms']\n",
    "\n",
    "            if method == 'nms':\n",
    "                iou_mask = iou > iou_threshold\n",
    "                weight[iou_mask] = 0.0\n",
    "\n",
    "            if method == 'soft-nms':\n",
    "                weight = np.exp(-(1.0 * iou ** 2 / sigma))\n",
    "\n",
    "            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n",
    "            score_mask = cls_bboxes[:, 4] > 0.\n",
    "            cls_bboxes = cls_bboxes[score_mask]\n",
    "\n",
    "    return best_bboxes\n",
    "\n",
    "\n",
    "def postprocess_boxes(pred_bbox, original_image, input_size, score_threshold):\n",
    "    valid_scale=[0, np.inf]\n",
    "    pred_bbox = np.array(pred_bbox)\n",
    "\n",
    "    pred_xywh = pred_bbox[:, 0:4]\n",
    "    pred_conf = pred_bbox[:, 4]\n",
    "    pred_prob = pred_bbox[:, 5:]\n",
    "\n",
    "    # 1. (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n",
    "    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n",
    "                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n",
    "    # 2. (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n",
    "    org_h, org_w = original_image.shape[:2]\n",
    "    resize_ratio = min(input_size / org_w, input_size / org_h)\n",
    "\n",
    "    dw = (input_size - resize_ratio * org_w) / 2\n",
    "    dh = (input_size - resize_ratio * org_h) / 2\n",
    "\n",
    "    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n",
    "    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n",
    "\n",
    "    # 3. clip some boxes those are out of range\n",
    "    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n",
    "                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n",
    "    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n",
    "    pred_coor[invalid_mask] = 0\n",
    "\n",
    "    # 4. discard some invalid boxes\n",
    "    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n",
    "    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n",
    "\n",
    "    # 5. discard boxes with low scores\n",
    "    classes = np.argmax(pred_prob, axis=-1)\n",
    "    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n",
    "    score_mask = scores > score_threshold\n",
    "    mask = np.logical_and(scale_mask, score_mask)\n",
    "    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n",
    "\n",
    "    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)\n",
    "\n",
    "\n",
    "def detect_image(Yolo, original_image, input_size=416, CLASSES=YOLO_COCO_CLASSES, score_threshold=0.3, iou_threshold=0.45, rectangle_colors=''):\n",
    "    image_data = image_preprocess(np.copy(original_image), [input_size, input_size])\n",
    "    image_data = image_data[np.newaxis, ...].astype(np.float32)\n",
    "    \n",
    "    pred_bbox = Yolo.predict(image_data)\n",
    "        \n",
    "    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n",
    "    pred_bbox = tf.concat(pred_bbox, axis=0)\n",
    "    \n",
    "    bboxes = postprocess_boxes(pred_bbox, original_image, input_size, score_threshold)\n",
    "    #bboxes = nms(bboxes, iou_threshold, method='nms')\n",
    "    print(bboxes)\n",
    "    image = draw_bbox(original_image, bboxes, CLASSES=CLASSES, rectangle_colors=rectangle_colors)\n",
    "        \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX0TGlJhMGd_"
   },
   "source": [
    "**Test by loading trained model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16498,
     "status": "ok",
     "timestamp": 1601446652047,
     "user": {
      "displayName": "Python Lessons",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgMQmMhFapKcavl337-vY17yrbowBHBlZQ5qYQv=s64",
      "userId": "12382394757900236362"
     },
     "user_tz": -180
    },
    "id": "NUKLydfYCo4r"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from yolov3.configs import *\n",
    "import random\n",
    "\n",
    "label_txt = \"objects/objects_test.txt\"\n",
    "labels = open(label_txt).readlines()\n",
    "index = 18\n",
    "model = Create_Yolov3(input_size=YOLO_INPUT_SIZE, CLASSES=TRAIN_CLASSES)\n",
    "model.load_weights(f\"./checkpoints/{TRAIN_MODEL_NAME}\") # use keras weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_info = labels[index].split()\n",
    "\n",
    "image_path = image_info[0]\n",
    "original_image      = cv2.imread(image_path) * 2\n",
    "\n",
    "image = detect_image(model, original_image, input_size=YOLO_INPUT_SIZE, CLASSES=TRAIN_CLASSES, rectangle_colors=(255,0,0), iou_threshold = 0.1)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(30,15))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdpGgKUUMJOe"
   },
   "source": [
    "**Test by testing detection on original model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.layers import Activation, Concatenate, Add, ZeroPadding2D, LeakyReLU, BatchNormalization, Conv2D, Input, MaxPooling2D, Dropout, concatenate, UpSampling2D, ReLU, InputLayer, Conv2DTranspose, DepthwiseConv2D\n",
    "\n",
    "model_dir = \"A:/models/yolo/\"\n",
    "model_name = \"yolo\"\n",
    "\n",
    "epsilon = np.finfo(float).eps\n",
    "lines_in_graph = []\n",
    "\n",
    "model_data_dir = os.path.join(model_dir, model_name)\n",
    "if not os.path.exists(model_data_dir):\n",
    "    os.makedirs(model_data_dir)\n",
    "    \n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, \":\", layer.__class__.__name__, layer.name)\n",
    "    items_to_write = [layer.__class__.__name__, layer.name]\n",
    "    if isinstance(layer,InputLayer):\n",
    "        inpu_shape = layer.input.shape\n",
    "        items_to_write.append(\"\")\n",
    "        items_to_write.append(str(inpu_shape[2]))\n",
    "        items_to_write.append(str(inpu_shape[1]))\n",
    "        items_to_write.append(str(inpu_shape[3]))\n",
    "    elif isinstance(layer.input, list):\n",
    "        input_layer_names = []\n",
    "        for i in range(len(layer.input)):\n",
    "            inpu_layer_name = layer.input[i].name.split('/')[0]\n",
    "            input_layer_names.append(inpu_layer_name)\n",
    "        items_to_write.append(\"&\".join(input_layer_names))\n",
    "        if isinstance(layer, Add):\n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,Concatenate): \n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "            '''\n",
    "            elif isinstance(layer,HeadWrapper):\n",
    "                for i in range(len(layer.input)):\n",
    "                    inpu_shape = layer.input[i].shape\n",
    "                    items_to_write.append(str(inpu_shape[2]))\n",
    "                    items_to_write.append(str(inpu_shape[1]))\n",
    "                    items_to_write.append(str(inpu_shape[3])) '''\n",
    "        else:\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"Error: Multi input layer not known\")\n",
    "            print(\"---------------------------------\")            \n",
    "    else:\n",
    "        inpu_layer_name = layer.input.name.split('/')[0]\n",
    "        items_to_write.append(inpu_layer_name)\n",
    "        if isinstance(layer,Conv2D):\n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            layer_activation = layer.activation\n",
    "            if layer_activation:\n",
    "                items_to_write.append(layer_activation.__name__)\n",
    "            else:\n",
    "                print(\"Warning: Activation layer present : \", layer_activation.__name__)                \n",
    "                print(\"---------------------------------\")\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            items_to_write.append(str(weights_shape[3]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            dilation_rates = layer.dilation_rate\n",
    "            if dilation_rates[0] != dilation_rates[1] and dilation_rates[1] != 1:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: dilation_rates must be equal\")\n",
    "                print(\"---------------------------------\")                \n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "            items_to_write.append(str(layer.padding))            \n",
    "        elif isinstance(layer,Conv2DTranspose):\n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            items_to_write.append(str(weights_shape[3]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "        elif isinstance(layer,DepthwiseConv2D):   \n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "        elif isinstance(layer,BatchNormalization):\n",
    "            weights = layer.get_weights()\n",
    "            gamma = weights[0]\n",
    "            beta = weights[1]\n",
    "            moving_mean = weights[2]\n",
    "            moving_variance = weights[3]\n",
    "            valueToSqrt = np.sqrt(moving_variance + epsilon)\n",
    "            a = gamma / np.sqrt(valueToSqrt)\n",
    "            b = - moving_mean * a + beta\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_mean.npy')\n",
    "            np.save(path_to_save, b)\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_variance.npy')\n",
    "            np.save(path_to_save, a) \n",
    "\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,MaxPooling2D):\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,ZeroPadding2D):\n",
    "            if(layer.padding[0][1] != layer.padding[1][1] and layer.padding[0][0] == 0 and layer.padding[1][0] != 0):\n",
    "                print(layer.padding)\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: This padding not supported\") \n",
    "                print(\"---------------------------------\")\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            items_to_write.append(str(layer.padding[0][0]))\n",
    "            items_to_write.append(str(layer.padding[0][1]))\n",
    "            items_to_write.append(str(layer.padding[1][0]))\n",
    "            items_to_write.append(str(layer.padding[1][1]))\n",
    "        elif isinstance(layer, ReLU):\n",
    "            maxValue = layer.max_value\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            items_to_write.append(str(maxValue))\n",
    "        elif isinstance(layer, LeakyReLU):\n",
    "            alpha = layer.alpha.item()\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            items_to_write.append(str(alpha))\n",
    "        elif isinstance(layer, Activation):\n",
    "            activation_type = layer.get_config()['activation']\n",
    "            maxValue = 0\n",
    "            if activation_type == 'relu':\n",
    "                items_to_write.append(\"relu\")\n",
    "                maxValue = 0\n",
    "                if hasattr(layer,'max_value'):\n",
    "                    maxValue = layer.max_value\n",
    "                inpu_shape = layer.input.shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "                items_to_write.append(str(maxValue))\n",
    "            elif activation_type == 'sigmoid':\n",
    "                items_to_write.append(\"sigmoid\")\n",
    "                inpu_shape = layer.input.shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "            elif activation_type == 'softmax':\n",
    "                items_to_write.append(\"softmax\")\n",
    "                inpu_shape = layer.input.shape\n",
    "                output_shape = layer.output.shape\n",
    "                print(output_shape)\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "            else:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Only Relu activation with max value supported, provided is \", activation_type)\n",
    "                print(\"---------------------------------\") \n",
    "        else:\n",
    "            print(layer.input.shape)\n",
    "            print(\"Error: Layer is not identified \", layer.__class__.__name__, \"-> \", layer.name)\n",
    "    lines_in_graph.append(\",\".join(items_to_write))\n",
    "with open(os.path.join(model_dir, f'{model_name}.txt'), 'w') as f:\n",
    "    # Write each line of the list to the file\n",
    "    for line in lines_in_graph:            \n",
    "        f.write(line + '\\n')\n",
    "    print(\"File is saved to \" + f'{model_name}.txt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "YOLOv3_colab_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
